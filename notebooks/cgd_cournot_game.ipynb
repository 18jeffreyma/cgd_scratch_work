{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:51:34.619542Z",
     "start_time": "2021-01-13T06:51:34.241436Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "\n",
    "from multi_cmd.optim import potentials, cmd_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Price and Cost\n",
    "\n",
    "**TLDR: linear price function, identical linear cost function, pairwise CGD converges to Nash Equilibrium**\n",
    "\n",
    "Our profit for each player $i$ is defined as the following:\n",
    "\\begin{gather}\n",
    "\\Pi_i = P\\left(\\sum_j{q_j}\\right) \\cdot q_i -C_i(q_i) \\\\\n",
    "P(q) = 100 - q \\\\\n",
    "C_i(q_i) = 10 \\cdot q_i\n",
    "\\end{gather}\n",
    "\n",
    "Thus, to solve for the Nash equilbrium, we take the first derivative and set it to zero:\n",
    "\\begin{gather}\n",
    "\\frac{\\partial\\Pi_i}{\\partial q_i} = \\frac{\\partial P\\left(\\sum_j{q_j}\\right)}{\\partial q_i} \\cdot q_i + P\\left(\\sum_j{q_j}\\right) - \\frac{\\partial C_i (q_i)}{\\partial q_i} = 0\n",
    "\\end{gather}\n",
    "\n",
    "For the example below, this becomes the following:\n",
    "\\begin{gather}\n",
    "-1 \\cdot q_i + \\left(100 - \\sum_j {q_j}\\right) - 10 = 0\n",
    "\\end{gather}\n",
    "\n",
    "Solving this analytically, we get $q_i = \\frac{45}{2}$ (which is what our algorithm converges to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:53:32.975441Z",
     "start_time": "2021-01-13T06:53:31.691951Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([22.5000], requires_grad=True), tensor([22.5001], requires_grad=True), tensor([22.5000], requires_grad=True)]\n",
      "[tensor([-506.2484], grad_fn=<NegBackward>), tensor([-506.2491], grad_fn=<NegBackward>), tensor([-506.2484], grad_fn=<NegBackward>)]\n"
     ]
    }
   ],
   "source": [
    "# Simple linear price, linear cost game.\n",
    "def player_payoffs(quantity_list):\n",
    "    quantity_tensor = torch.stack(quantity_list)\n",
    "    price = torch.max(100 - torch.sum(quantity_tensor),\n",
    "                      torch.tensor(0., requires_grad=True))\n",
    "                      \n",
    "    payoffs = []\n",
    "    for i, quantity in enumerate(quantity_tensor):\n",
    "        # Negative, since CGD minimizes player objectives.\n",
    "        payoffs.append(- (quantity * price - 10 * quantity))\n",
    "        \n",
    "    return payoffs\n",
    "\n",
    "# Number of iterations and setting up game.\n",
    "num_iterations = 100\n",
    "bregman = potentials.shannon_entropy(100)\n",
    "\n",
    "# Define individual sellers quantities\n",
    "p1 = torch.tensor([10.], requires_grad=True)\n",
    "p2 = torch.tensor([20.], requires_grad=True)\n",
    "p3 = torch.tensor([10.], requires_grad=True)\n",
    "\n",
    "player_list = [p1, p2, p3]\n",
    "\n",
    "optim = cmd_utils.CMD([[p1], [p2], [p3]], bregman=bregman)\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    payoffs = player_payoffs(player_list)\n",
    "    optim.step(payoffs)\n",
    "\n",
    "print(player_list)\n",
    "print(payoffs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Price Function\n",
    "\n",
    "**TLDR: quadratic price function, identical linear cost function, pairwise CGD converges to Nash Equilibrium (with learning rate tuning)**\n",
    "\n",
    "Our profit for each player $i$ is defined as the following:\n",
    "\\begin{gather}\n",
    "\\Pi_i = P\\left(\\sum_j{q_j}\\right) \\cdot q_i -C_i(q_i) \\\\\n",
    "P(q) = 100 - \\sum_j{q_j^2} \\\\\n",
    "C_i(q_i) = 10 \\cdot q_i\n",
    "\\end{gather}\n",
    "\n",
    "Thus, to solve for the Nash equilbrium, we take the first derivative and set it to zero:\n",
    "\\begin{gather}\n",
    "\\frac{\\partial\\Pi_i}{\\partial q_i} = \\frac{\\partial P\\left(\\sum_j{q_j}\\right)}{\\partial q_i} \\cdot q_i + P\\left(\\sum_j{q_j}\\right) - \\frac{\\partial C_i (q_i)}{\\partial q_i} = 0\n",
    "\\end{gather}\n",
    "\n",
    "For the example below, this becomes the following:\n",
    "\\begin{gather}\n",
    "-2 \\cdot q_i^2 + \\left(100 - \\sum_j {q_j^2}\\right) - 10 = 0 \\\\\n",
    "90 = \\sum_{i\\neq j} {q_j^2} - 3 q_i^2\n",
    "\\end{gather}\n",
    "\n",
    "Solving this, we have multiple Nash Equlibrium, but the only solution with all non-negative quantities (as defined by our bregman divergence constraints), we get $q_i = \\sqrt{18} = 4.24$ (which is what our algorithm converges to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:53:36.270039Z",
     "start_time": "2021-01-13T06:53:34.791330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([4.2426], requires_grad=True), tensor([4.2426], requires_grad=True), tensor([4.2426], requires_grad=True)]\n",
      "[tensor([-152.7349], grad_fn=<NegBackward>), tensor([-152.7354], grad_fn=<NegBackward>), tensor([-152.7349], grad_fn=<NegBackward>)]\n"
     ]
    }
   ],
   "source": [
    "# Simple quadratic price, linear cost game.\n",
    "def player_payoffs(quantity_list):\n",
    "    quantity_tensor = torch.stack(quantity_list)\n",
    "    price = torch.max(\n",
    "        100 - torch.sum(torch.pow(quantity_tensor, 2)),\n",
    "        torch.tensor(0., requires_grad=True)\n",
    "    )\n",
    "                      \n",
    "    payoffs = []\n",
    "    for i, quantity in enumerate(quantity_tensor):\n",
    "        # Negative, since CGD minimizes player objectives.\n",
    "        payoffs.append(- (quantity * price - 10 * quantity))\n",
    "        \n",
    "    return payoffs\n",
    "\n",
    "# Number of iterations and setting up game.\n",
    "num_iterations = 100\n",
    "bregman = potentials.shannon_entropy(100)\n",
    "\n",
    "# Define individual sellers quantities\n",
    "p1 = torch.tensor([1.], requires_grad=True)\n",
    "p2 = torch.tensor([2.], requires_grad=True)\n",
    "p3 = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "player_list = [p1, p2, p3]\n",
    "\n",
    "optim = cmd_utils.CMD([[p1], [p2], [p3]], bregman=bregman)\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    payoffs = player_payoffs(player_list)\n",
    "    optim.step(payoffs)\n",
    "\n",
    "print(player_list)\n",
    "print(payoffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Cost Function\n",
    "\n",
    "**TLDR: linear price function, identical non-linear cost function (simulating at-scale production), pairwise CGD converges to Nash Equilibrium**\n",
    "\n",
    "Our profit for each player $i$ is defined as the following:\n",
    "\\begin{gather}\n",
    "\\Pi_i = P\\left(\\sum_j{q_j}\\right) \\cdot q_i -C_i(q_i) \\\\\n",
    "P(q) = 100 - q \\\\\n",
    "C_i(q_i) = 10 \\cdot \\left(\\frac{10}{x+10}\\right)\n",
    "\\end{gather}\n",
    "\n",
    "Thus, to solve for the Nash equilbrium, we take the first derivative and set it to zero:\n",
    "\\begin{gather}\n",
    "\\frac{\\partial\\Pi_i}{\\partial q_i} = \\frac{\\partial P\\left(\\sum_j{q_j}\\right)}{\\partial q_i} \\cdot q_i + P\\left(\\sum_j{q_j}\\right) - \\frac{\\partial C_i (q_i)}{\\partial q_i} = 0\n",
    "\\end{gather}\n",
    "\n",
    "For the example below, this becomes the following:\n",
    "\\begin{gather}\n",
    "-1 \\cdot q_i + \\left(100 - \\sum_j {q_j}\\right) - \\frac{1000}{(q_i+10)^2} = 0\n",
    "\\end{gather}\n",
    "\n",
    "Solving this analytical equation (using MATLAB/Octave), we have multiple Nash Equilibrium, but the only solution with all non-negative quantities, we get $q_i = 24.793$ (which is what our algorithm converges to). \n",
    "\n",
    "```\n",
    "syms a positive\n",
    "syms b positive\n",
    "syms c positive\n",
    "\n",
    "[sol_a, sol_b, sol_c] = solve(...\n",
    "    100 - 2*a - b - c - 1000/((a+10)^2), ...\n",
    "    100 - a - 2*b - c - 1000/((b+10)^2), ...\n",
    "    100 - a - b - 2*c - 1000/((c+10)^2) ...\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:57:05.998112Z",
     "start_time": "2021-01-13T06:57:04.291113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([24.7935], requires_grad=True), tensor([24.7936], requires_grad=True), tensor([24.7935], requires_grad=True)]\n",
      "[tensor([-563.9376], grad_fn=<NegBackward>), tensor([-563.9387], grad_fn=<NegBackward>), tensor([-563.9376], grad_fn=<NegBackward>)]\n"
     ]
    }
   ],
   "source": [
    "# Simple quadratic price, linear cost game.\n",
    "def player_payoffs(quantity_list):\n",
    "    quantity_tensor = torch.stack(quantity_list)\n",
    "    price = torch.max(\n",
    "        100 - torch.sum(quantity_tensor),\n",
    "        torch.tensor(0., requires_grad=True)\n",
    "    )\n",
    "                      \n",
    "    payoffs = []\n",
    "    for i, quantity in enumerate(quantity_tensor):\n",
    "        # Negative, since CGD minimizes player objectives.\n",
    "        payoffs.append(- (quantity * price - 100 * quantity / (quantity + 10)))\n",
    "        \n",
    "    return payoffs\n",
    "\n",
    "# Number of iterations and setting up game.\n",
    "num_iterations = 100\n",
    "bregman = potentials.shannon_entropy(100)\n",
    "\n",
    "# Define individual sellers quantities\n",
    "p1 = torch.tensor([1.], requires_grad=True)\n",
    "p2 = torch.tensor([2.], requires_grad=True)\n",
    "p3 = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "player_list = [p1, p2, p3]\n",
    "\n",
    "optim = cmd_utils.CMD([[p1], [p2], [p3]], bregman=bregman)\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    payoffs = player_payoffs(player_list)\n",
    "    optim.step(payoffs)\n",
    "\n",
    "print(player_list)\n",
    "print(payoffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
