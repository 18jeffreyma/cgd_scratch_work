{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T02:16:37.126739Z",
     "start_time": "2020-11-12T02:16:36.805112Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cgd_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cournot Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Price and Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TLDR: linear price function, identical linear cost function, pairwise CGD converges to Nash Equilibrium**\n",
    "\n",
    "Our profit for each player $i$ is defined as the following:\n",
    "\\begin{gather}\n",
    "\\Pi_i = P\\left(\\sum_j{q_j}\\right) \\cdot q_i -C_i(q_i) \\\\\n",
    "P(q) = 100 - q \\\\\n",
    "C_i(q_i) = 10 \\cdot q_i\n",
    "\\end{gather}\n",
    "\n",
    "Thus, to solve for the Nash equilbrium, we take the first derivative and set it to zero:\n",
    "\\begin{gather}\n",
    "\\frac{\\partial\\Pi_i}{\\partial q_i} = \\frac{\\partial P\\left(\\sum_j{q_j}\\right)}{\\partial q_i} \\cdot q_i + P\\left(\\sum_j{q_j}\\right) - \\frac{\\partial C_i (q_i)}{\\partial q_i} = 0\n",
    "\\end{gather}\n",
    "\n",
    "For the example below, this becomes the following:\n",
    "\\begin{gather}\n",
    "-1 \\cdot q_i + \\left(100 - \\sum_j {q_j}\\right) - 10 = 0\n",
    "\\end{gather}\n",
    "\n",
    "Solving this, we get $q_i = \\frac{45}{2}$ (which is what our algorithm converges to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T02:46:45.245172Z",
     "start_time": "2020-11-12T02:46:43.738878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22.5002],\n",
      "        [22.4998],\n",
      "        [22.5001]], grad_fn=<StackBackward>)\n",
      "tensor([[-506.2539],\n",
      "        [-506.2442],\n",
      "        [-506.2520]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "def player_payoffs(quantity_tensor,\n",
    "                   market_demand=lambda q: 100 - q,\n",
    "                   marginal_cost=lambda q: q * 10):\n",
    "    price = torch.max(market_demand(torch.sum(quantity_tensor)),\n",
    "                      torch.tensor(0., requires_grad=True))\n",
    "\n",
    "    payoffs = []\n",
    "    for i, quantity in enumerate(quantity_tensor):\n",
    "        # Negative, since CGD minimizes player objectives.\n",
    "        payoffs.append(- (quantity * price - marginal_cost(quantity)))\n",
    "        \n",
    "    return torch.stack(payoffs)\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "# Define individual sellers quantities\n",
    "p1 = torch.tensor([50.], requires_grad=True)\n",
    "p2 = torch.tensor([0.], requires_grad=True)\n",
    "p3 = torch.tensor([40.], requires_grad=True)\n",
    "\n",
    "players = torch.stack([p1, p2, p3])\n",
    "\n",
    "learning_rates = [0.1, 0.1, 0.1]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    payoffs = player_payoffs(players)\n",
    "    updates, _ = cgd_utils.metamatrix_conjugate_gradient(\n",
    "        payoffs, [p1, p2, p3], lr_list=learning_rates)\n",
    "    \n",
    "    for player, update in zip(players, updates):\n",
    "        player.data.add_(update)\n",
    "\n",
    "print(players)\n",
    "print(payoffs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Price Function\n",
    "\n",
    "**TLDR: quadratic price function, identical linear cost function, pairwise CGD converges to Nash Equilibrium (with learning rate tuning)**\n",
    "\n",
    "Our profit for each player $i$ is defined as the following:\n",
    "\\begin{gather}\n",
    "\\Pi_i = P\\left(\\sum_j{q_j}\\right) \\cdot q_i -C_i(q_i) \\\\\n",
    "P(q) = 100 - \\sum_j{q_j^2} \\\\\n",
    "C_i(q_i) = 10 \\cdot q_i\n",
    "\\end{gather}\n",
    "\n",
    "Thus, to solve for the Nash equilbrium, we take the first derivative and set it to zero:\n",
    "\\begin{gather}\n",
    "\\frac{\\partial\\Pi_i}{\\partial q_i} = \\frac{\\partial P\\left(\\sum_j{q_j}\\right)}{\\partial q_i} \\cdot q_i + P\\left(\\sum_j{q_j}\\right) - \\frac{\\partial C_i (q_i)}{\\partial q_i} = 0\n",
    "\\end{gather}\n",
    "\n",
    "For the example below, this becomes the following:\n",
    "\\begin{gather}\n",
    "-2 \\cdot q_i^2 + \\left(100 - \\sum_j {q_j^2}\\right) - 10 = 0\n",
    "\\end{gather}\n",
    "\n",
    "Solving this, we have multiple Nash Equlibrium, but the only solution with all non-negative quantities, we get $q_i = \\sqrt{18} = 4.24$ (which is what our algorithm converges to).\n",
    "\n",
    "A few things to note, pairwise CGD here seems to have convergence rely more on learning rate (i.e. diverging for larger LR), which maybe defeats some of the core purpose of CGD. However, we can see that this behavior diverges into territory outside of our game constraints (i.e. negative quantities), so adding constraints (like in CMD) might fix this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T04:45:08.445820Z",
     "start_time": "2020-11-12T04:45:07.035006Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.2426],\n",
      "        [4.2426],\n",
      "        [4.2426]], grad_fn=<StackBackward>)\n",
      "tensor([[-152.7349],\n",
      "        [-152.7351],\n",
      "        [-152.7351]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "def player_payoffs2(quantity_tensor,\n",
    "                   market_demand=lambda q: 100 - q,\n",
    "                   marginal_cost=lambda q: q * 10):\n",
    "    price = torch.max(\n",
    "        market_demand(torch.sum(torch.pow(quantity_tensor, 2))),\n",
    "        torch.tensor(0., requires_grad=True)\n",
    "    )\n",
    "\n",
    "    payoffs = []\n",
    "    for i, quantity in enumerate(quantity_tensor):\n",
    "        # Negative, since CGD minimizes player objectives.\n",
    "        payoffs.append(- (quantity * price - marginal_cost(quantity)))\n",
    "        \n",
    "    return torch.stack(payoffs)\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "# Define individual sellers quantities\n",
    "p1 = torch.tensor([0.], requires_grad=True)\n",
    "p2 = torch.tensor([7.], requires_grad=True)\n",
    "p3 = torch.tensor([7.], requires_grad=True)\n",
    "\n",
    "players = torch.stack([p1, p2, p3])\n",
    "\n",
    "learning_rates = [0.01, 0.01, 0.01]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    payoffs = player_payoffs2(players)\n",
    "    updates, _ = cgd_utils.metamatrix_conjugate_gradient(\n",
    "        payoffs, [p1, p2, p3], lr_list=learning_rates)\n",
    "    \n",
    "    for player, update in zip(players, updates):\n",
    "        player.data.add_(update)\n",
    "\n",
    "print(players)\n",
    "print(payoffs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Cost Function\n",
    "\n",
    "**TLDR: linear price function, identical non-linear cost function (simulating at-scale production), pairwise CGD converges to Nash Equilibrium**\n",
    "\n",
    "Our profit for each player $i$ is defined as the following:\n",
    "\\begin{gather}\n",
    "\\Pi_i = P\\left(\\sum_j{q_j}\\right) \\cdot q_i -C_i(q_i) \\\\\n",
    "P(q) = 100 - q \\\\\n",
    "C_i(q_i) = 10 \\cdot \\left(\\frac{10}{x+10}\\right)\n",
    "\\end{gather}\n",
    "\n",
    "Thus, to solve for the Nash equilbrium, we take the first derivative and set it to zero:\n",
    "\\begin{gather}\n",
    "\\frac{\\partial\\Pi_i}{\\partial q_i} = \\frac{\\partial P\\left(\\sum_j{q_j}\\right)}{\\partial q_i} \\cdot q_i + P\\left(\\sum_j{q_j}\\right) - \\frac{\\partial C_i (q_i)}{\\partial q_i} = 0\n",
    "\\end{gather}\n",
    "\n",
    "For the example below, this becomes the following:\n",
    "\\begin{gather}\n",
    "-1 \\cdot q_i + \\left(100 - \\sum_j {q_j}\\right) - \\frac{1000}{(q_i+10)^2} = 0\n",
    "\\end{gather}\n",
    "\n",
    "Solving this, we have multiple Nash Equilibrium, but the only solution with all non-negative quantities, we get $q_i = \\sqrt{18} = 4.24$ (which is what our algorithm converges to).\n",
    "\n",
    "A few things to note, pairwise CGD here seems to have convergence rely more on learning rate (i.e. diverging for larger LR), which maybe defeats some of the core purpose of CGD. However, we can see that this behavior diverges into territory outside of our game constraints (i.e. negative quantities), so adding constraints (like in CMD) might fix this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T05:52:22.895760Z",
     "start_time": "2020-11-12T05:52:22.239798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33.1543],\n",
      "        [33.1544]], grad_fn=<StackBackward>)\n",
      "tensor([[-1040.1848],\n",
      "        [-1040.1873]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "def player_payoffs3(quantity_tensor,\n",
    "                   market_demand=lambda q: 100 - q,\n",
    "                   marginal_cost=lambda q: 100 * q / (q + 10)):\n",
    "    price = torch.max(\n",
    "        market_demand(torch.sum(quantity_tensor)),\n",
    "        torch.tensor(0., requires_grad=True)\n",
    "    )\n",
    "\n",
    "    payoffs = []\n",
    "    for i, quantity in enumerate(quantity_tensor):\n",
    "        # Negative, since CGD minimizes player objectives.\n",
    "        payoffs.append(- (quantity * price - marginal_cost(quantity)))\n",
    "        \n",
    "    return torch.stack(payoffs)\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "# Define individual sellers quantities\n",
    "p1 = torch.tensor([0.], requires_grad=True)\n",
    "p2 = torch.tensor([5.], requires_grad=True)\n",
    "# p3 = torch.tensor([10.], requires_grad=True)\n",
    "\n",
    "players = torch.stack([p1, p2])\n",
    "\n",
    "learning_rates = [0.1, 0.1]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    payoffs = player_payoffs3(players)\n",
    "    updates, _ = cgd_utils.metamatrix_conjugate_gradient(\n",
    "        payoffs, [p1, p2], lr_list=learning_rates)\n",
    "    \n",
    "    for player, update in zip(players, updates):\n",
    "        player.data.add_(update)\n",
    "\n",
    "print(players)\n",
    "print(payoffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
